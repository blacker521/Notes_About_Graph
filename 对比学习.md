---
typora-root-url: ./
---

# 2021.3.3
## [What Makes for Good Views for Contrastive Learning](https://arxiv.org/pdf/2005.10243.pdf) 
主要对比了几种常见的对比学习，为什么能取得比较好的效果。
在保持与下游任务相关的信息的同时，减少视图之间的MI，才能取得比较好的效果。
数据增强是一种有效减小不同视图之间的MI的一种方法
建立视图不变表示的能力是研究核心
**对比表示学习的最佳视图是依赖于任务的**
如果共享信息小，则学习表示可以对输入的详细信息，并实现对多余变量不变的程度更大。

### 提出了三个定义

1. **Sufficient Encoder **
	![](.\img\c_1001.png)
2. **Minimal Sufficient Encoder. **
	![](.\img\c_1002.png)
3. **Optimal Representation of a Task.**
	![](.\img\c_1003.png)
### 用实验证明了对于下游任务，对比与下游相关的特征的重要性
![](.\img\c_1004.png)
上图表示，对于视图v1中的xt进行数据增强，可以控制两个 view 之间共享的变量，分别共享位置，数字，背景。
![](.\img\c_1005.png)

1. **表示不同view之间共享的信息对下游任务影响很大**
	比如只共享数字的部分，学习到的特征表示会忽略背景和位置信息，因此在有关位置和背景的下游任务中无法取得比较好的效果。其他的也类似。
2. **当共享多个信息时，往往其中一个信息会占主导地位**
	如共享数字和位置时，数字占据了主要地位，可能是因为卷积本来就不对位置敏感。
### 学习分为三个阶段
![](.\img\c_1006.png)
这样会形成一个U型，最高点定义为甜点，我们的目标就是让两个视图的信息能够刚好达到甜点，不多不少，只学到特定的特征。
![](.\img\c_1007.png)
![](.\img\c_1008.png)
![](.\img\c_1009.png)

### 总结

**在最小化两个视图之间与任务相关的互信息，最大化两个视图和特征之间的互信息**

对特定任务要有不用不同两个视图，有没有能够让下游任务都很好的特征？
运用到图上的话主要问题还在怎样对数据增强的问题？
特征学习是一个反U型，在图像上可以用色彩抖动来消除，图上应该怎样做？还是说这种问题解决不了？


## [Contrastive Multi-View Representation Learning on Graphs](http://proceedings.mlr.press/v119/hassani20a/hassani20a.pdf) 
在图上用最大化互信息MI来学习表示，主要延伸GraphInfoMax 和SimSLR，又有孪生网路的结构又用了对比互信息。
最大化从图的不同结构视图编码的表示之间的MI来引入自监督方法来训练图编码器。

### 观点
1. 将视图数量增加到两个以上不会提高性能，就用两个对比就行
2. 通过对比来自一阶邻居的编码和图扩散这两个视图，可以实现最佳性能
3. 跨视图对比节点和图的编码能够取得更好的结果
4. 与分层图池化方法相比，简单的图读出层在两项任务上均具有更好的性能
5. 应用正则化或归一化对性能有负面的影响
### 模型
![](.\img\c_2001.png)
1. 节点级和图级进行多视图表示学习
2. 图扩散用于生成样本图和其他结构视图。样本图是同一图的相关视图，仅应用于图结构，不应用于初始节点。并进行子图采样。
3. 用两个专用的GNN和MLP（共享权重）学习节点级的表示。
4. 池化层和MLP层为读出功能，读出图级表示。
5. 鉴别器将一个视图的节点表示与另一视图的图级表示进行对比学习。
### 数据增强
**数据增强可以分为两种：**
1. 对初始节点特征进行操作的特征空间增强，例如，掩盖或添加高斯噪声
2. 通过添加或删除连接性，子采样或使用最短距离或扩散矩阵生成全局视图，对图结构进行操作的结构空间扩充和破坏。
3. 在大多数情况下，将邻接矩阵转换为扩散矩阵并将这两个矩阵视为同一图结构的两个全等视图，即可获得最佳结果。我们推测是因为邻接矩阵和扩散矩阵提供了，图结构的局部和全局视图分别最大化了从这两个视图中获悉的表示形式之间的一致性，从而使模型可以同时对丰富的局部和全局信息进行编码。
### 扩散矩阵定义
![](.\img\c_2002.png)

### 读出函数

将每个GCN层中节点表示的总和串联起来，然后将其馈送到单层前馈网络，以使节点表示与图形表示之间的尺寸大小保持一致。g表示全局的表示

![](.\img\c_2003.png)
### Loss
![](.\img\c_2010.png)
### 伪代码
![](.\img\c_2011.png)
### 总结
这篇工作不是太好，没有比较好的特点，只是用了两个视角，把图像上的东西搬过来，但是模型还是有些建设性意义的。

## [DEEP GRAPH INFOMAX](https://arxiv.org/abs/1809.10341)

DGI是一种无监督方式学习图结构节点表示的通用方法，依赖于最大化局部表示与相应的全局表示之间的互信息。DGI不依赖于随机游走，因为随机游走以图结构信息为目标，过分强调邻近信息，而且依赖于超参数。
定义的全局是局部信息的拼接
**DGI是DIM在图上的应用**
DIM训练编码器模型，以最大化高级“全局”表示和输入的“局部”（例如图像的补丁）之间的相互信息。 鼓励编码器携带在所有位置都存在的信息类型（因此是全局相关的），例如类别标签的情况。**比卷积捕获的结构更加通用**
对比方法的关键实现细节是如何绘制正样本和负样本。 上面关于无监督图表示学习的先前工作依赖于局部对比损失（强制近端节点具有相似的嵌入）。

**这样对比能学习到远距离的结构相似信息。目标是通过图建立相似节点间的链接**
**学习出一个encoder**

### 模型
1. 通过腐蚀函数对负样本进行采样
2. 通过将输入图传给编码器来获得表示hi（正样本）
3. 通过负样本传给编码器来获得表示hj（负样本）
4. 通过读出函数获得全局的表示（平均正样本的表示）
5. 通过判别器和Loss进行更新
![](.\img\c_2004.png)
![](.\img\c_2005.png)
### 局部特征
![](.\img\c_2006.png)
### 全局特征
![](.\img\c_2007.png)
### 获得负样本表示
![](.\img\c_2008.png)
### 区分正负样本对（判别器）
![](.\img\c_2009.png)
### 总结
这篇论文是DMI在Graph上的一个应用，取得了比较好的效果

## [Learning deep representations by mutual information estimation and maximization](https://arxiv.org/abs/1808.06670) 

通过最大化深度神经网络编码器的输入输之间的互信息来进行无监督的表示学习
**将输入中有关局部性的知识纳入目标，可以显著提高表示形式对下游任务的适用性**
对比学习的鼻祖论文，第一次将MI用作对比损失
由于MINE的问世使得MI能够准确的估算，这也使得通过估算MI来进行计算对比损失较为容易。
**结构信息是非常重要的。利用MI进行表示学习，根据下游任务，最大化表示和输入局部区域之间的平均MI，可以极大地提高例如分类任务的表示质量**
在完整的输入和编码器输出（即全局MI）之间最大化MI通常不足以学习有用的表示
引入了两种新的表示质量度量，一种基于互信息神经估计一种基于神经依赖性度量（NDM）
图1是经典的编码器，把高维的特征编码成低维的向量
![](.\img\c_3001.png)
图2是DMI，高维特征编码成特征图之后压缩成高级特征向量，低级的特征图经过判别器进行打分并和高级特征向量加和
**根据下游任务，在原始的X或者X的子图上进行最大化**
![](.\img\c_3002.png)
最大化本地要素和全局要素之间的相互信息。 首先，我们将图像编码为一个特征图，以反映数据的某些结构方面，例如 空间局部性，我们将这个特征图进一步概括为一个全局特征向量（参见图1）。 然后，我们将该特征向量与每个位置的低级特征图连接起来。 通过附加功能为每个局部-全局对生成一个分数。

以图像分类为例，我们可以将猫图像x编码为$f(x)∈R^{M×M×d}$，并提取局部特征向量$v∈R^d$。 为了在实例和上下文之间进行对比，我们还需要另外两件事：

• 一个汇总函数$g:R^{M×M×d}→R^d$, 生成上下文向量$s = g(f(x))∈R^d$

• 另一个猫图像$ x − $(负样本)及其上下文向量$s{-} = g(f(x))$。

然后将对比目标表述为:

$\mathcal{L}=\mathbb{E}_{v, x}\left[-\log \left(\frac{e^{v^{T} \cdot s}}{e^{v^{T} \cdot s}+e^{v^{T} \cdot s^{-}}}\right)\right]$

# 2020.3.4
## [InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization](https://arxiv.org/abs/1908.01000)
论文研究在无监督和半监督情况下学习整个图的表示（图级）
DGI是节点级的预测
最大化图级表示和不同比例的子结构表示（例如节点，边，三角形）之间的相互信息
图形级表示就对跨不同比例的子结构共享的数据各方面进行编码
分为无监督版InfoGraph和半监督版InfoGraph\*
许多图分析任务（例如图分类，回归和聚类）要求将整个图表示为固定长度的特征向量。 尽管可以通过隐式地通过节点级表示获得图形级表示，但是显式提取图形对于面向图形的任务可能更直接，更优化。
在半监督学习中，将大量未标记的样本与少量标记的样本合并在一起以提高模型的准确性，将在这些领域中发挥关键作用。

### 无监督型
![](.\img\c_4001.png)

用卷积学到图级表示后,对一个视图的表示进行合并获得高级的特征表示,并和低级表示输入鉴别器,鉴别器将[全局表示,局部表示]对,作为输入,并确定他们是不是来自同一个图。InfoGraph使用批处理的方式生成所有可能的正样本和负样本。
如图所示，例子中有2个输入图（一个批次就这两个）总共生成了7个局部表示，对于蓝色的全局表示，将有7个输入对输入到鉴别器。对于红色的也是如此（红色作为全局表示）
在这种情况下，鉴别器将采用14对[全局，局部]对作为输入。
### 函数设置
首先用GNN学习到局部的表示，，k表层数，v表示节点v，e是边。通过最大化图级之间的相互信息来获得图表示和局部级别的表示形式。
![](.\img\c_4002.png)
再用一个读出函数将所有的局部表示合成为固定长度的图级表示
![](.\img\c_4003.png)
用最大互信息进行对比，u是一个子图
![](.\img\c_4004.png)
用JSD估计的MI和DGI一样
![](.\img\c_4005.png)

###　半监督模型
有两种具有相同体系结构的独立编码器，一种用于监督任务，另一种使用带有未监督目标的带标记和未标记数据（等式4）进行训练。
经过读出函数的都是高级特征表示
![](.\img\c_4006.png)
损失函数为三部分，监督，无监督，MI
![](.\img\c_4007.png)

### 总结
这篇论文主要argue了之前论文没有图级的表示，图级别的分类
用子图的方式去做图分类
虽然说的是图级别的，但是他还是用GCN对结构中的每个节点都进行了编码再合并得到了一个高级的特征向量
感觉和之前的Contrastive Multi-View Representation Learning on Graphs没啥区别。只是那篇文章对输入进行了增强然后用了SimSLR那中孪生框架，这篇文章是用一个网络。
# 2021.3.5
## [SelfMatch: Combining Contrastive Self-Supervision and Consistency for Semi-Supervised Learning](https://arxiv.org/abs/2101.06480)
###  模型SelfMatch
SelfMatch包含两个阶段：
1. 基于对比学习的自我监督预训练。SimCLR
2. 基于增强一致性正则化的半监督微调。FixMatch
![](.\img\c_5001.png)
### 总结
东西没啥，就是两个原来的工作和在了一起。

## [What Should Not Be Contrastive in Contrastive Learning](https://arxiv.org/abs/2008.05659)
引入了一个对比学习框架，该框架不需要事先特定的，与任务相关的不变性的知识，模型学会捕捉通过构建单独的视觉表示的可变和不变因素嵌入到空间，除了扩充之外，每个空间都是不变的。
**数据增强的引入是一把双刃剑**因为每次数据增强都会增加其中的不变性变多，比如，增加旋转可能会有助于航拍图像识别，但是会大大降低解决其他任务的能力。
**在不假定下游不变性的先验知识的情况下，在对比性学习框架中捕获个体变化因素的表示**
不是将图像映射到对所有手工扩增都不变的单个嵌入空间，而是学习构造单独的嵌入子空间，每个子空间对特定扩增敏感，而对其他扩增不敏感。
旨在以统一的表示形式保留有关每个扩增的信息，并学习它们的不变性。然后，可以将经过这些扩充训练的一般表示形式应用于不同的下游任务。
**不需要手动选择数据增强策略**
![](.\img\c_6001.png)
使用“留一法”策略生成多个视图，然后将它们的表示形式投影到具有对比目标的单独的嵌入空间中，其中每个嵌入空间对于所有扩充都是不变的，或者除了一个扩充之外，其余都是不变的。 学习的表示形式可以是一般嵌入空间V（蓝色区域），也可以是嵌入子空间Z（灰色区域）的串联。
随机扩增的任何视图相同实例的模块T映射到嵌入空间中的同一点上。该属性对学习的表示形式有负面影响：
1. 如果将通用性和可传递性应用于必须丢弃的信息是必不可少的任务则会损害通用性和可传递性，例如颜色在鸟类的细粒度分类中起着重要作用
2. 增加额外的扩充十分复杂，因为新的运算符可能对某些类别有帮助，而对其他类别则有害，例如，旋转的花朵可能与原始花朵非常相似，而对于旋转的汽车则不适用； 
3. 控制增强强度的超参数需要针对每种增强进行仔细调整，以在保持快捷方式开放和完全使一个信息源无效之间达到微妙的平衡。

###  LooC 留一法对比学习

框架可以有选择地防止由于扩充而导致的信息丢失，而不是将每个视图都投影到一个不变于所有扩充的嵌入空间中，在LooC方法中，输入图像的表示被投影到多个嵌入空间中，每个嵌入空间都对于当前扩充改变，而对其他扩充则保持不变。这样，每个嵌入子空间专用于单个扩充，并且共享层将同时包含可变不变信息和不变信息，与几个嵌入空间共同共享表示； 我们要么将共享表示形式转移，要么将所有空间的连接转移给下游任务。
![](.\img\c_6001.png)
![](.\img\c_6002.png)
![](.\img\c_6003.png)
n个增强n+1个z空间，z0是所有的增强都在一起，其他的是不同的增强，每个增强一个嵌入空间

### 总结
本篇论文主要是说，原来的对比学习都是映射到同一空间，但是这样会有害学习其他的特征，所以他把每个特征都映射到单独的特征空间，这个空间里都只有经过这一种数据增强的数据。
其实说白了还是在说数据增强是要根据下游任务来说的，分成不同的嵌入空间来适合多种不同的下游任务，但是对需要两种以上特征的下游任务效果可能就不好了。比如不仅仅需要结构信息，还需要位置。也不能再去把两个不同的增强再去学习一遍。

## [Contrastive and Generative Graph Convolutional Networks for Graph-based Semi-Supervised Learning](https://arxiv.org/abs/2009.07111)
以GCN基础的半监督学习，基于图的半监督学习（SSL）旨在通过图将少数标记数据的标签转移到剩余的大量未标记数据。
首先，通过设计一个半监督的对比损失，可以通过最大化相同数据的不同视图或同一类数据之间的一致性来产生改进的节点表示，因此，丰富的未标记数据和稀缺而有价值的数据标记数据可以共同提供丰富的监督信息，以学习判别性节点表示形式，有助于改善后续的分类结果。
其次，通过使用与输入特征相关的图形生成损失，提取数据特征与输入图形拓扑之间的潜在确定性关系作为SSL的补充监管信号。
**设计了一种新的半监督对比损失，该方法将类别信息附加到了改善节点分类任务的对比表示学习。**
分别获得从全局视图和局部视图生成的节点表示，然后采用半监督的对比损失来最大化从这两个视图学习的表示之间的一致性。
其次，考虑到**图拓扑**本身包含宝贵的信息，可以将其用作SSL的补充监督信号，因此我们使用了一个生成项来**显式地建模图和节点表示之间的关系**。结果，可以通过探索来自数据相似性和图形结构的知识来进一步扩展标记数据的最初有限的监督信息，从而导致改进的数据表示和分类结果。
**通过从两个视图在所获得的表示之间进行对比学习，可以同时对丰富的全局信息和局部信息进行编码**
用GCN学习，从局部视图获取节点表示
![](.\img\c_7001.png)
φ1表示局部视图
用HGCN学习全局的表示，将结构相似的节点重复聚合成超节点φ2表示全局视图
HGCN将结构相似的图节点重复聚合到一组超节点上，这可以生成用于卷积的粗化图并扩大节点的接收场。 然后，应用对称图精炼层以恢复原始图结构，以进行节点级表示学习。 这种分层图卷积模型可以从本地到全局的角度全面捕获节点的信息。

###  模型
![](.\img\c_7002.png)

一张图复制有一份，一份经过GCN生成局部视角的表示，另一部分通过HGCN生成全局视角的表示。对比两个视角之间的损失。

损失 = 半监督对比损失 + 生成损失 + 交叉熵损失
#### 半监督对比损失
![](.\img\c_7003.png)
![](.\img\c_7004.png)
### 生成损失（为了学习图拓扑结构）
![](.\img\c_7006.png)
### 总损失
![](.\img\c_7008.png)
### 伪代码
![](.\img\c_7007.png)
### 总结
这篇论文用了两个视角的对比学习，加入了图拓扑信息。
# 2021.3.6
## [Exploring Simple Siamese Representation Learning](https://arxiv.org/abs/2011.10566)

我们的模型无需使用负对或动量编码器，就可以直接最大程度地最大化一幅图像两个视图的相似度。
**没有动量编码器的BYOL**
**没有负对的SimCLR**
**没有在线聚类的SwAV**
可以⾃然地引⼊归纳偏差来对不变性进⾏建摸。
"不变性"意味着具有相同概念的两个观测值应产⽣相同的输出。

### 模型
![](.\img\c_8002.png)

1. 将一幅图像（image x）放大为两个放大视图（x1，x2）。
2. x1，x2同时经过编码器（encoder f）处理得到f（x1）和f（x2），该encoder f由卷积神经网络和全连接层组成，网络结构相同且共享权重。
3. 一侧应用预测MLP h（predictor h），将f（x1）转换为h（f（x1））。
4. 另一侧应用停止梯度运算。
5. 使两者之间的相似性最大化。

### 损失
![](.\img\c_8003.png)
### 伪代码
![](.\img\c_8001.png)

## [Graph Contrastive Learning with Augmentations](https://arxiv.org/abs/2010.13902)

图增强对比学习

孪生网络框架，用于学习数据的无监督学习表示。

设计了四种不同的图扩充方法

对比学习目的：通过不同扩充视图下最大化特征一致性来学习表示形式。

![](.\img\c_9001.png)
![](.\img\c_9002.png)
![](.\img\c_9003.png)

### 总结
基于孪生网络的对比学习，提出了图数据上的增强方式

## **对比学习正负例在干什么？**

原始图片被当作一种anchor，其增强的图片被当作正样本（positive sample），然后其余的图片被当作负样本。

解决拼图问题是无监督学习中一个非常重要的部分。在对比学习中，原图被当作anchor，打乱后的图片被当作正样本，其余图片被当作负样本

![](.\img\c_10001.png)
![](.\img\c_10002.png)
![](.\img\c_10003.png)

## **对比学习中的互信息？**

上下文实例对比度主要有两种类型：**预测相对位置（PRP）和最大化互信息（MI）**。 它们之间的区别是：

• PRP重点学习局部成分之间的**相对位置**。 全局上下文是预测这些关系的隐含要求（例如了解大象的长相对于预测其头尾之间的相对位置至关重要）。

• MI关注学习**局部和全局内容之间关系的显式信息**。 局部成分之间的相对位置将被忽略。

![](.\img\c_10005.png)
![](.\img\c_10006.png)
![](.\img\c_10007.png)
![](.\img\c_10008.png)
![](.\img\c_10009.png)

## 根据如何采样负样本来将对比学习进行分类

![](.\img\c_10010.png)

（a）端到端训练，一个encoder用来生成正样本的表示，一个encoder用来生成负样本的表示；大的batch size来存储更多的负样本，使用一个对比损失，模型会让正样本的表示相近，让负样本和正样本的表示相远。SimSLR

（b）使用一个Memory bank来存储和抽取负样本；**Memory bank**：的作用是在训练的时候维护大量的负样本表示。所以，创建一个字典来存储和更新这些样本的嵌入。Memory bank $M$ 在数据集$D$中对每一个样本$I$存储一个表示$m_i$。该机制可以更新负样本表示，而无需增大训练的batch size。

（c）使用一个momentum encoder当作一个动态的字典查询来处理负样本；momentum创建了一种特殊的字典，它把字典当作一个队列的keys，当前的batch进入队列，最老的batch退出队列。Momentum encoder 共享了encoder Q的参数。它不会在每次反向传播后更新, 而是依据query encoder的参数来更新：
$$
\theta_{k} \leftarrow m \theta_{k}+(1-m) \theta_{q}
$$
在公式中, $m \in[0,1)$ 是momentum系数。只有参数 $\theta_{q}$ 会被反向传播更新。
Momentum update使得 $\theta_{k}$ 缓慢、柔和地依据 $\theta_{q}$ 来更新, 使得两个encoders的区别并不会很大。
Momentum encoder的优点是不需要训练两个不一样的encoder, 而且维护memory bank会比较简单。

（d）额外使用一个聚类机制。两个共享参数的端到端架构，这种架构使用聚类算法来聚类相似样本表示。

## Loss

为了训练一个encoder, 需要一个前置任务来利用对比损失来进行反向传播。
对比学习最核心的观点是将相似样本靠近，不相似样本靠远。
所以需要一个相似度衡量指标来衡量两个表示的相近程度。
在对比学习中，最常用的指标是cosine similarity。
$$
\cos _{s} i m(A, B)=\frac{A \cdot B}{\|A\|\|B\|}
$$
Noise Contrastive Estimation (NCE)  函数定义为:
$$
L_{N C E}=-\log \frac{\exp \left(\operatorname{sim}\left(q, k_{+}\right) / \tau\right.}{\exp \left(\operatorname{sim}\left(q, k_{+}\right) / \tau\right)+\exp \left(\operatorname{sim}\left(q, k_{-}\right) / \tau\right)}
$$
其中 $q$ 是原样本, $k_{+}$ 是正样本, $k_{-}$ 是负样本, $\tau$ 是超参数，被称为温度系数。 $\operatorname{sim}()$ 可以是任何相似度函数，一般来说是cosines similarity
如果负样本的数量很多, NCE的一个变种 InfoNCE 定义为：
$$
L_{I n f o N C E}=-\log \frac{\exp \left(\operatorname{sim}\left(q, k_{+}\right) / \tau\right.}{\exp \left(\operatorname{sim}\left(q, k_{+}\right) / \tau\right)+\sum_{i=0}^{K} \exp \left(\operatorname{sim}\left(q, k_{i}\right) / \tau\right)}
$$
$k_{i}$ 表示负样本。
与其他深度学习模型类似, 对比学习应用了许多训练优化算法。训练的过程包括最小化损失函数来学习模型的参数。常见的优化算法包括 SGD 和 Adam 等。训练大的 batch 的网络有时需要特殊设计的优化算法，例如 LARS

# 2021.3.26

## [Deep Graph Contrastive Representation Learning](https://arxiv.org/abs/2006.04131)

利用节点级别的对比目标

最大化两个视图(属性级和结构级)中节点表示的一致性来学习节点表示

提高输入节点特征和高级节点嵌入之间的MI

主要侧重于对比节点级别的嵌入

正负对进行对比,王亮老师组的关于对比学习论文,还有一篇自适应的GCA,模型结构和这篇差不多

### 模型

![](/img/c_11001.png)

首先从一个图分别用删边和Mask的方法生成,两个视图,对于一个视图内:目标节点与其他节点生成视图内的负样本对,在视图间,目标节点和另一个视图中除了anchor的其他节点成为负例,anchor成为正例

**Enconder**

用GNN作为Enconder

**数据增强方法**

删边和Mask

### Loss

![](/img/c_11002.png)

其中: $\theta(\boldsymbol{u}, \boldsymbol{v})=s(g(\boldsymbol{u}), g(\boldsymbol{v}))$,s是余弦相似度计算,g是非线性的投影层(两层MLP),能够增强表达能力

**总的Loss**

$\mathcal{J}=\frac{1}{2 N} \sum_{i=1}^{N}\left[\ell\left(\boldsymbol{u}_{i}, \boldsymbol{v}_{i}\right)+\ell\left(\boldsymbol{v}_{i}, \boldsymbol{u}_{i}\right)\right]$

相当于一张图形成两个视角,跑了两遍,计算了平均Loss,反向传播

### 代码

看了遍代码,感觉这个代码的Loss有些问题

# [Graph Contrastive Learning with Adaptive Augmentation](https://arxiv.org/abs/2010.14945)

自适应增强图对比学习

结合图的拓扑和语义方面的信息

 数据增强方案应保留图形的固有结构信息和属性信息,使模型的学习对不重要的节点和边的扰动不敏感的表示形式

 通过对比学习学习到的表示形式往往不会因数据增强方案而导致损坏

增强方法是应适应输入图,(且适应下游任务)

### GCA

![](/img/c_12001.png)

边的重要性用集中度衡量 

# [Motif-Driven Contrastive Learning of Graph Representations](https://arxiv.org/abs/2012.12533)

Motif驱动的图表示对比学习

对于大型数据集而言挖掘和利用Motif很难,作者提出了一种预训练Motif+对比学习子图的方式学习.

给定一个数据集,Motif学习器将相似且重要的子图合成相应的Motif集并基于Motif集来训练子图分割,子图将用于对比学习.

## Motivation

可以使用GNN从大型图形数据集中自动提取图形主题吗？==>

可以利用学习到的图形主题来受益于自我监督的GNN学习吗？==>用学习到的Motif集来指导GNN进行学习

精确计数和抽样统计估计是原始方法,无法用在大型数据集上==>提出了基于GNN的可分化聚类学习问题

## Model

由于Motif可以通过其性质来表示关键图的属性，因此利用学习到的Motif来生成更多信息性的子图视图。通过对比学习学习这种子图共现，经过预训练的GNN可以捕获节点级对比无法捕获的图的更高层信息。

Motif学习器通过应用期望最大化（EM）样式聚类来学习主题采样子图上的算法。

EM算法是一种迭代优化策略，由于它的计算方法中每一次迭代都分两步，其中一个为期望步（E步），另一个为极大步（M步），所以算法被称为EM算法（Expectation Maximization Algorithm）

![](/img/c_13001.png)

1. 输入图经过GNN编码后得到节点的嵌入表示和整图的嵌入表示.

2. Embedding经过Motif提取器提取重要的Motif集

3. Motif集经过子图Pooling生成子图的表示

4. 子图的表示一面用来和整图进行对比学习,加强学习到的表示,另一方面,学习一个Motif分类器,对Motif集进行分类,并用来指导Motif提取器提取重要的Motif

## 总结

这篇论文结合了Motif和GNN来学习到Motif集,并用来指导GNN提取重要的Motif

加了个对比学习,但是感觉是没啥用的,和Motif没啥关系,还是子图和整图去对比.

有没有一种方法能把Motif和整图对比,学习比较好的Motif表示,这些表示能够可解释?对于对齐来说能后解释由于哪些Motif或者结构影响了这次对齐

没中WWW21'论文不太漂亮

