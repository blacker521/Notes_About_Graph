今天主要讲下动态的社交网络对齐，动态的社交网络对齐就是给定一个动态增长的网络在不降低准确率的情况下尽可能更快速的计算。所以不能用传统的静态对齐的方法每个单独的时间片都去对齐，这样很耗费时间。

一个直观的想法是只训练每次新增的节点和其相邻的节点，比如图中，橙色节点是新增节点，在T1时刻，用传统的对齐算法进行计算，计算出所有节点的Emb，当T2时刻，对新加入的节点，肯定会和T1时刻的图有连边，用这些边和节点，计算新加入节点的Emb，但是这种方法存在这一些问题，比如T2时刻学习到新节点的表示不太好，没有很强的鲁棒性，那T2之后时刻会累计这种影响，使对齐的准确率越来越低。在结束时的准确率会大大降低。

而且还存在一个问题，在T1时刻我训练好了节点的表示，同过y = theta　x将两个网络中的节点进行了对齐，每个Net1中的点都可以通过这个函数找到在Net2中找到一个唯一确定的点，这样我们就说他是对齐的，这个函数可以叫做对齐函数，把网络1中的点对齐到网络2中，找到跟他对齐的节点。

在T2时刻，仍然可以学习到这样一个对齐函数，这个对齐函数是只用了新加入的节点的子图。

在T3时刻呢？这个对齐函数再用新加入的节点去进行对齐？本来更新的Embding就有一定的问题， 那对齐函数学到的全重可能也是有问题的。

要怎样解决这样的问题呢，提高鲁棒性呢？思考了三种方案

第一种方案是用Transformer的想法，Emb更新的策略不变，还是用子图去更新新加入子图的Emb，但是在对齐函数上，用N个时间片，学到N-1个对齐函数的参数权重，输入到Transformer中，让他们之间尽可能的相似，从而提高泛化性能，但是又不能很相似，所以加入了GAN去加一些扰动，这块代码有点难写，还在改。

然后最近做比赛又有了几个想法，第一个就是用对比学习，在更新Emb的时候就增加他的鲁棒性，对新加入的节点随机游走出几条路径，进行对比学习，一个节点可能会走出很多不同的路径或者子图，把他们放入对比学习框架，最后取平均，来更新节点的表示，从而提高鲁棒性。

还有一个就是，用NLP的方法，比如，我静态图可以看作是一个文本，一个只有主谓宾的句子，然后动态图就可以看做是加了形容词或者是定语。那我的对齐就可以是将两个网络对齐的任务映射为两个文本匹配相似的度或者翻译的问题的问题，就是有没有可能将Bert预训练的向量加到入到我的网络之中，去做下游任务的微调，

或者说我直接用T1时刻的网络去预训练Emb，以后时刻的节再通过这个网络学习到Emb，而不是用子图来更新，大家共享一个网络参数层，然后在不同时刻进行微调，这样来提高鲁棒性。

