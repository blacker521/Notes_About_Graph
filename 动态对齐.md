## 现存问题

1. 动态社交网络对齐时，有一种想法是每个时刻都重新训练，每次都重新训练，但是这样每次都进行训练是比较消耗资源的

现在的想法是只训练增加的数据

但是增加的节点怎样和之前的节点进行联系是一个问题，如果只用新加入节点和原始节点相连的边进行更新向量表示直观上是不好的

**比如对于共同作者网络，在T1时刻作者A和作者B发表了一篇论文，A是大老板（度很大），B是小老板（度比较大），但是在T2时刻B这小老板已经可以独立发表论文了，但是没有了A这个共同作者，但是A对这些作者还是有影响的。**子图能解决这个问题吗？把二阶邻居的子图都提取出来?

这样网络是不具有鲁棒性的（当新加入节点后，重新训练的参数是不正确的，只学到了部分）

有多少个时间段就要训练多少个W来进行对齐，但是每个时刻学到的W应该是不同的（因为有新节点的加入）一样的话就被泛化了，缺少了鲁棒性

### 方法一

用Transformer  self Attention编码W，能有效的增强鲁棒性，让每个时间段的W都尽可能的相似

用LSTM学习时序信息，讲学到的两种Emd取平均

### 方法二

用类似预训练的方法，先训练一套参数，在每个新加入的点上进行微调

先用T0时刻的大网络训练参数，在每次更新网络的时候进行微调。

这种可能需要去用图映射到文字（节点度数----->词频------>Bert预训练向量---->微调）

对齐任务--->翻译（两种语句的句向量相似度）

对齐任务---->文本匹配（判断两个句子是不是说的同一件事儿，两个随机游走序列是不是相同）

### 方法三

主要是做IJCA想到的想法，现在IJCA上做对齐主要的想法是用Bert接一个分类器，Bert共用，分类器是微调的，在对齐上是不是能用

对于每个anchor建立一个微调的分类器，这个两个网络中anchor连接的几个点做分类，新加入的节点看是不是

### 方案四

用对比学习的思想，原来想的是用整个网络去对比，学到更好的表示，如果拿子图过来对比学习，

